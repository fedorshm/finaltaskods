# [Natural Language Processing course (stream 7, autumn 2024)](https://ods.ai/tracks/nlp-course-autumn-2024)
## Финальное задание
Суть проекта - выявить разницу между качеством генерации заголовков новостных статей (задача суммаризации) между более "мощной" моделью ruGPT-3 Medium (350 млн параметров), дообученной на меньшем кол-ве данных (6900 статей), и более "слабой" моделью ruGPT-3 Small (125 млн параметров), дообученной на большем кол-ве данных (15000 статей).

Результат: более "слабые" модели ruGPT-3 Small, обученные на большем кол-ве данных, показывают значительно более высокий результат (согласно метрике ROUGE), чем более "мощные" модели ruGPT-3 Medium, обученные на меньшем количестве данных.

В таблице ниже представлены средние значения F-мер, полученные в результате оценки сгенерированных моделями заголовков метрикой ROUGE. Значения показывают, насколько в среднем процентов сгенерированные заголовки похожи на оригинальные. "Наука", "Спорт" и "Наука+Спорт" - названия моделей по датасетам новостей, на которых они были дообучены (всего 6 моделей). 

|  | RuGPT-3 Medium | RuGPT-3 Small |
|----------|----------|----------|
| Наука    | 17,6    | 23,8    |
| Спорт    | 19,9    | 32,8    |
| Наука+Спорт    | 22    | 27,5    |

## Ссылки на модели в HuggingFace:

### Обученные на "слабой" ruGPT-3 Small (125 млн параметров) на "больших" датасетах по 15,000 статей:

- [Наука](https://huggingface.co/Gnider/nauka_6ep_15k)
- [Спорт](https://huggingface.co/Gnider/sport_6ep_15k)
- [Наука + Спорт](https://huggingface.co/Gnider/mix_6ep_15k)

### Обученные на "сильной" ruGPT-3 Medium (350 млн параметров) на "маленьких" датасетах по 6,900 статей:

- [Наука](https://huggingface.co/Gnider/nauka_6900_6ep_17_600_rugptmedium)
- [Спорт](https://huggingface.co/Gnider/sport_6900_6ep_17_600_rugpt3medium)
- [Наука + Спорт](https://huggingface.co/Gnider/mix_6900_6ep_17_600_tugpt3medium)

## Файлы репозитория

- Датасеты, на которых производилось дообучение и анализ: datasets_6900.zip, datasets_15k_part1.zip и datasets_15k_part2.zip (датасеты по 15к разделены, потому что вместе GitHub не пропускал их по размеру)
- gpt_training_config.json - конфиг для дообучения ruGPT
- train.py - py-файл для дообучения
- ноутбук kaggle - блокнот с подготовкой данных для дообучения, дообучением и оценкой ROUGE
